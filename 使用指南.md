## 配置环境

### pull docker
```
docker pull rouqian/bevformer:mgc2012_latest
```

### 创建pkl数据集文件

```
python tools/create_data.py nuscenes --root-path ./data/nuscenes --out-dir ./data/nuscenes --extra-tag nuscenes
```
注意:  
运行时可能会报错, 可以先用v1.0-mini测试下, 如果运行成功再使用trainval  

修改的地方如下:  
位置: tools/data_converter/nuscenes_converter.py  
的84-103行osp.join(info_prefix, ....) -> osp.join(root_path, ...)


### 单卡调试
使用单卡调试  
```python tools/train.py configs/nuscenes/det/centerhead/lssfpn/camera/256x704/swint/default.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth --no-dist```  

或者使用vscode  
```json
{
    "name": "cam-only debug",
    "type": "python",
    "request": "launch",
    "program": "${workspaceFolder}/tools/train.py",
    "console": "integratedTerminal",
    "env": {
        "CUDA_VISIBLE_DEBICES": "0",
    },
    "args": [
        "configs/nuscenes/det/centerhead/lssfpn/camera/256x704/swint/default.yaml",
        "--model.encoders.camera.backbone.init_cfg.checkpoint",
        "pretrained/swint-nuimages-pretrained.pth",
        "--no-dist"
    ],  
    "justMyCode": true
}
```




### Evaluation

Download [pretrained models](https://drive.google.com/drive/folders/1Jru7VTfgvFF949DlP1oDfriP3wrmOd3c)  


Then, you will be able to run:

```bash
torchpack dist-run -np 8 python tools/test.py [config file path] pretrained/[checkpoint name].pth --eval [evaluation type]
```

For example, if you want to evaluate the detection variant of BEVFusion, you can try:

```bash
torchpack dist-run -np 8 python tools/test.py configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml pretrained/bevfusion-det.pth --eval bbox
```

While for the segmentation variant of BEVFusion, this command will be helpful:

```bash
torchpack dist-run -np 8 python tools/test.py configs/nuscenes/seg/fusion-bev256d2-lss.yaml pretrained/bevfusion-seg.pth --eval map
```

### Training

We provide instructions to reproduce our results on nuScenes.

For example, if you want to train the camera-only variant for object detection, please run:

```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/det/centerhead/lssfpn/camera/256x704/swint/default.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth
```

For camera-only BEV segmentation model, please run:

```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/seg/camera-bev256d2.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth
```

For LiDAR-only detector, please run:

```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/det/transfusion/secfpn/lidar/voxelnet_0p075.yaml
```

For LiDAR-only BEV segmentation model, please run:

```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/seg/lidar-centerpoint-bev128.yaml
```

For BEVFusion detection model, please run:
```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/det/transfusion/secfpn/camera+lidar/swint_v0p075/convfuser.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth --load_from pretrained/lidar-only-det.pth 
```

For BEVFusion segmentation model, please run:
```bash
torchpack dist-run -np 8 python tools/train.py configs/nuscenes/seg/fusion-bev256d2-lss.yaml --model.encoders.camera.backbone.init_cfg.checkpoint pretrained/swint-nuimages-pretrained.pth
```

Note: please run `tools/test.py` separately after training to get the final evaluation metrics.
注意: 使用单卡调试时, 需将

### 可能会遇到的问题
min_lr_ratio  
- https://github.com/mit-han-lab/bevfusion/issues/479
evaluation encounters mpirun: not found  
- https://github.com/mit-han-lab/bevfusion/issues/21


### 可能会用到的
停止训练进程后kill占用GPU  
`ps -ef |grep "python tools/train.py" | cut -c 11-16|xargs kill -9`